{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9bf7ea",
   "metadata": {},
   "source": [
    "## Sanitize ISO doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f3ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/henriscaffidi/Documents/Work/SHL/Public_ISO23726-1/.venv/lib/python3.13/site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/henriscaffidi/Documents/Work/SHL/Public_ISO23726-1/.venv/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/henriscaffidi/Documents/Work/SHL/Public_ISO23726-1/.venv/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: html2text in /Users/henriscaffidi/Documents/Work/SHL/Public_ISO23726-1/.venv/lib/python3.13/site-packages (2025.4.15)\n"
     ]
    }
   ],
   "source": [
    "# Installs the required Python packages for web scraping and HTML parsing.\n",
    "\n",
    "!pip install beautifulsoup4\n",
    "!pip install html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import difflib\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54dce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open HTML\n",
    "\n",
    "if not os.path.exists('docs/p1.html'):\n",
    "    with open('docs/p1.html', 'w', encoding='utf-8') as f:\n",
    "        f.write('<html><body>Forward ... A complete listing of these bodies can be found at www.iso.org/members.html.</body></html>')\n",
    "\n",
    "with open('docs/p1.html', 'r', encoding='utf-8') as f:\n",
    "    html = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9edaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the entire section under the header 'Foreword', preserving 'Introduction' and all that follows\n",
    "\n",
    "pattern1 = r'(<h1 class=\"sts-sec-title\">Foreword</h1>.*?)(?=<h1 class=\"sts-sec-title\">Introduction</h1>)'\n",
    "clean_html = re.sub(pattern1, '', html, flags=re.DOTALL)\n",
    "\n",
    "# Remove the instructional text under the Terms and Definitions header\n",
    "\n",
    "pattern2 = r'(<div class=\"sts-p\"><i>The Terms and definitions clause is a mandatory element of the text.</i></div>.*?)(?=<h2 class=\"sts-sec-title\">3.1&nbsp;&nbsp;&nbsp;Types of ontologies</h2>)'\n",
    "clean_html = re.sub(pattern2, '', clean_html, flags=re.DOTALL)\n",
    "\n",
    "# Remove the section from 'Annex A' header to the 'Bibliography' header\n",
    "\n",
    "pattern3 = r'(<h1 class=\"sts-app-header\">Annex A</h1>.*?)(?=<h2 class=\"sts-sec-title\">Bibliography</h2>)'\n",
    "clean_html = re.sub(pattern3, '', clean_html, flags=re.DOTALL)\n",
    "\n",
    "# Replace the text in the 'Normative references' section with \"There are no normative references\"\n",
    "def replace_normative_references(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find all top-level sts-section divs\n",
    "    for section in soup.find_all('div', class_='sts-section'):\n",
    "        h1 = section.find('h1', class_='sts-sec-title')\n",
    "        if h1 and 'Normative references' in h1.get_text():\n",
    "            # Remove all children except the h1\n",
    "            for child in section.find_all(recursive=False):\n",
    "                if child != h1:\n",
    "                    child.decompose()\n",
    "            # Add the replacement paragraph\n",
    "            new_para = soup.new_tag('div', attrs={\"class\": \"sts-p\"})\n",
    "            new_para.string = \"There are no normative references.\"\n",
    "            section.append(new_para)\n",
    "            break  # Only replace the first matching section\n",
    "\n",
    "    return str(soup)\n",
    " \n",
    "clean_html = replace_normative_references(clean_html)\n",
    "\n",
    "# Replace the text in the 'Parts in the OBI series' section with \"New text\"\n",
    "def replace_parts(html):\n",
    "    pattern = re.compile(\n",
    "        r'(<h1 class=\"sts-sec-title\"[^>]*>4\\s*&nbsp;&nbsp;&nbsp;Parts in the OBI series</h1>.*?)(?=<h1 class=\"sts-sec-title\"|$)',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    def replacer(match):\n",
    "        header = re.search(r'<h1 class=\"sts-sec-title\"[^>]*>4\\s*&nbsp;&nbsp;&nbsp;Parts in the OBI series</h1>', match.group(1))\n",
    "        if header:\n",
    "            # Make the URL clickable\n",
    "            new_text = (\n",
    "                'This document is Part 1 of the multi-part ISO 23726 OBI series. '\n",
    "                'Part 2 Vocabulary is development and in the ISO process. '\n",
    "                'Part 3 is the foundation of the OBI series and describes the Industrial Data Ontology. '\n",
    "                'IDO is used for representing industrial data and information, building vocabularies, and managing asset models which employ reference data libraries and exploit OWL DL. '\n",
    "                'IDO supports automated machine reasoning, data quality checks, and information models used in all life cycle phases of industrial systems, processes, and products. '\n",
    "                'The ISO standard for IDO is in development in ISO and due for public release in 2026. '\n",
    "                'A draft of the IDO standard document and a download of the digital artefact (current at the start of the ISO process in 2024) can be downloaded from the PoscCaesar web site at '\n",
    "                '<a href=\"https://rds.posccaesar.org/ontology/lis14/\">https://rds.posccaesar.org/ontology/lis14/</a>. '\n",
    "                'Other parts of OBI are envisaged and Part 100, the Schedule Data Ontology, is already in development in ISO. '\n",
    "                'Guideline documents for practical implementation are also being developed and made available by industry associations.'\n",
    "            )\n",
    "            return f'{header.group(0)}\\n<div class=\"sts-p\">{new_text}</div>\\n'\n",
    "        return match.group(1)\n",
    "    return pattern.sub(replacer, html)\n",
    " \n",
    "clean_html = replace_parts(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add header/title to the document\n",
    "title_html = '<h1 class=\"sts-sec-title\">Ontology-based Interoperability for Industrial Data</h1>\\n'\n",
    "\n",
    "# Add new section after the title\n",
    "new_section_html = '''\n",
    "<h3>Lead authors: Melinda Hodkiewicz and Andreas Neumann</h3>\n",
    "<h3>Development team: PÃ¥l Rylandsholm, Maja Milicic Brandt, Johan W Kluwer, Caitlin Woods, Dirk Walter, Inghild Kaarstad</h3>\n",
    "<h2>Abstract</h2>\n",
    "<div class=\"sts-p\">This document provides guidance to industry users and the semantic data modelling community on 1) the vision for the ISO 23726 Ontology-based Interoperability (OBI) series, and 2) a set of principles which resources will have to comply with in order to be considered <code>compliant</code> with IDO and the ISO 23726 series. The <a href=\"https://rds.posccaesar.org/ontology/lis14/\">Industrial Data Ontology (IDO)</a> is the upper ontology in the ISO 23726 series. IDO is currently inside the ISO process and due to be published as an ISO standard in 2026.<br><br>\n",
    "The contents of this document will be submitted to ISO as part of ISO 23726-1 in October 2025. The standardisation process is expected to take 3 years. During this period the contents of this document will evolve as other organisations and national bodies work to shape the ideas presented in this initial version. Once inside the ISO process only members of the ISO TC184/SC4 WG26 committee and the liaison groups will have access to the draft standard and any associated digital artefacts until it is published in 2027/2028.\n",
    "</div>\n",
    "<h2>Licence</h2>\n",
    "<div class=\"sts-p\"><a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC-BY-SA-4.0 licence</a></div>\n",
    "'''\n",
    "\n",
    "clean_html = title_html + new_section_html + clean_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97672f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove metadata from the document\n",
    "# Remove all HTML comments\n",
    "clean_html = re.sub(r'<!--.*?-->', '', clean_html, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds figures (see README for instructions)\n",
    "\n",
    "fig_counter = 1\n",
    "\n",
    "# Match: ./docs/p1_files/graphic-<whatever>.png\n",
    "pattern = re.compile(r'\\./p1_files/graphic-[^\"]+\\.png')\n",
    "\n",
    "def rename_graphic_path(match):\n",
    "    global fig_counter\n",
    "    new_path = f'./figs/Fig{fig_counter}.png'\n",
    "    fig_counter += 1\n",
    "    return new_path\n",
    "\n",
    "clean_html = pattern.sub(rename_graphic_path, clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'document' to 'draft' in the \"Scope\" section\n",
    "\n",
    "def replace_document_with_draft_in_scope(html):\n",
    "    scope_block_pattern = re.compile(\n",
    "        r'(<div class=\"sts-section\"[^>]*?>\\s*<h1 class=\"sts-sec-title\">[^<]*Scope[^<]*</h1>.*?</div>\\s*</div>)',\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    def replace_in_scope_block(match):\n",
    "        scope_html = match.group(1)\n",
    "        scope_html = re.sub(r'\\bDocuments\\b', 'Drafts', scope_html)\n",
    "        scope_html = re.sub(r'\\bdocuments\\b', 'drafts', scope_html)\n",
    "        scope_html = re.sub(r'\\bDocument\\b', 'Draft', scope_html)\n",
    "        scope_html = re.sub(r'\\bdocument\\b', 'draft', scope_html)\n",
    "        return scope_html\n",
    "\n",
    "    return scope_block_pattern.sub(replace_in_scope_block, html)\n",
    "\n",
    "clean_html = replace_document_with_draft_in_scope(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specific section that contains the ISO/TC 184/SC 4 committee information\n",
    "\n",
    "pattern = re.compile(\n",
    "    r'<div class=\"sts-p\">The ISO/TC 184/SC 4 committee.*?ontology-based\\s*interoperable ecosystem\\.\\s*</div>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "clean_html = re.sub(pattern, '', clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect sd.iso.org hrefs to existing term IDs\n",
    "\n",
    "def redirect_links_to_existing_term_ids(html_string, log_path=\"link_redirection_log.txt\"):\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "    # Step 1: Build map of term text (preferred + admitted) â section ID\n",
    "    term_to_id = {}\n",
    "    for section in soup.find_all('div', class_='sts-tbx-sec'):\n",
    "        section_id = section.get('id')\n",
    "        if not section_id:\n",
    "            continue\n",
    "\n",
    "        for term_class in ['preferredTerm', 'admittedTerm']:\n",
    "            for term_elem in section.find_all(class_=term_class):\n",
    "                term_text = term_elem.get_text(strip=True).lower()\n",
    "                term_text = re.sub(r'\\(.*?\\)', '', term_text).strip()\n",
    "                if term_text:\n",
    "                    term_to_id[term_text] = section_id\n",
    "\n",
    "    log_lines = []\n",
    "\n",
    "    # Step 2: Redirect all external hrefs to internal term matches\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if 'sd.iso.org' not in a['href']:\n",
    "            continue\n",
    "\n",
    "        anchor_text = a.get_text(strip=True).lower()\n",
    "        anchor_text_cleaned = re.sub(r'\\(.*?\\)', '', anchor_text).strip()\n",
    "\n",
    "        matched_id = term_to_id.get(anchor_text_cleaned)\n",
    "        if matched_id:\n",
    "            a['href'] = f'#{matched_id}'\n",
    "        else:\n",
    "            match = difflib.get_close_matches(anchor_text_cleaned, term_to_id.keys(), n=1, cutoff=0.85)\n",
    "            if match:\n",
    "                matched_id = term_to_id[match[0]]\n",
    "                a['href'] = f'#{matched_id}'\n",
    "                log_lines.append(f\"[Fuzzy Match] '{anchor_text}' â '{match[0]}' â #{matched_id}\")\n",
    "            else:\n",
    "                log_lines.append(f\"[Unmatched â removed] '{anchor_text}'\")\n",
    "                a.unwrap()\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "clean_html = redirect_links_to_existing_term_ids(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7483910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Table of Contents (TOC) for the document\n",
    "\n",
    "def generate_toc(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    if not headings:\n",
    "        return html  # nothing to do\n",
    "\n",
    "    # Skip the first <h1> (page title)\n",
    "    headings = headings[1:] if headings[0].name == 'h1' else headings\n",
    "\n",
    "    toc_soup = BeautifulSoup('<div id=\"toc\"><h2>Table of Contents</h2></div>', 'html.parser')\n",
    "    toc_div = toc_soup.div\n",
    "    current_level = 0\n",
    "    stack = [toc_soup.new_tag('ul')]\n",
    "    toc_div.append(stack[0])\n",
    "\n",
    "    for heading in headings:\n",
    "        level = int(heading.name[1])\n",
    "        if not heading.has_attr('id'):\n",
    "            heading_id = heading.text.strip().lower().replace(' ', '-')\n",
    "            heading['id'] = heading_id\n",
    "        else:\n",
    "            heading_id = heading['id']\n",
    "\n",
    "        while level > current_level:\n",
    "            new_ul = toc_soup.new_tag('ul')\n",
    "            stack[-1].append(new_ul)\n",
    "            stack.append(new_ul)\n",
    "            current_level += 1\n",
    "        while level < current_level:\n",
    "            stack.pop()\n",
    "            current_level -= 1\n",
    "\n",
    "        li = toc_soup.new_tag('li')\n",
    "        a = toc_soup.new_tag('a', href=f'#{heading_id}')\n",
    "        a.string = heading.text.strip()\n",
    "        li.append(a)\n",
    "        stack[-1].append(li)\n",
    "\n",
    "    # Insert TOC after the first <h1> (title), or at top of <body> if not found\n",
    "    first_h1 = soup.find('h1')\n",
    "    if first_h1:\n",
    "        first_h1.insert_after(toc_div)\n",
    "    elif soup.body:\n",
    "        soup.body.insert(0, toc_div)\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "clean_html = generate_toc(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add breakline before figure captions\n",
    "\n",
    "def add_space_before_labels(html_string):\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "    target_classes = ['sts-caption-label']\n",
    "\n",
    "    for class_name in target_classes:\n",
    "        for label in soup.find_all(class_=class_name):\n",
    "            prev = label.find_previous_sibling()\n",
    "\n",
    "            # Skip if already has spacing before\n",
    "            if prev and (\n",
    "                prev.name == 'br' or\n",
    "                (prev.name in ['p', 'div', 'section'] and prev.get_text(strip=True)) or\n",
    "                (prev.string and prev.string.strip() == '')\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            # Insert a <br> before the label\n",
    "            br_tag = soup.new_tag('br')\n",
    "            label.insert_before(br_tag)\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "clean_html = add_space_before_labels(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change definition headings to <h3> tags\n",
    "\n",
    "def convert_label_divs_to_h3(clean_html):\n",
    "    soup = BeautifulSoup(clean_html, 'html.parser')\n",
    "\n",
    "    # Find all divs with class 'sts-tbx-label'\n",
    "    for label_div in soup.find_all('div', class_='sts-tbx-label'):\n",
    "        # Replace <div> with <h3>\n",
    "        h3 = soup.new_tag('h3')\n",
    "        h3.string = label_div.get_text(strip=True)\n",
    "        label_div.replace_with(h3)\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "clean_html = convert_label_divs_to_h3(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74177208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned HTML\n",
    "with open('docs/p1_clean.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6446d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cleaned HTML to Markdown\n",
    "\n",
    "def normalize_anchor(value):\n",
    "    return re.sub(r'\\s+', '-', value.replace('\\xa0', '').strip())\n",
    "\n",
    "def preprocess_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    div_id_placeholders = []\n",
    "\n",
    "    # 1. Remove the Table of Contents div\n",
    "    toc_div = soup.find('div', id='toc')\n",
    "    if toc_div:\n",
    "        toc_div.decompose()\n",
    "\n",
    "    # 2. Replace all <a> with Markdown links\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href', '')\n",
    "        text = a.get_text().strip()\n",
    "        if not href or not text:\n",
    "            continue\n",
    "        if href.startswith('#'):\n",
    "            href = f\"#{normalize_anchor(href[1:])}\"\n",
    "        markdown_link = f\"[{text}]({href})\"\n",
    "        a.replace_with(markdown_link)\n",
    "\n",
    "    # 3. Normalize all id attributes\n",
    "    for tag in soup.find_all(attrs={\"id\": True}):\n",
    "        tag['id'] = normalize_anchor(tag['id'])\n",
    "\n",
    "    # 4. Replace each sts-tbx-sec div with a placeholder\n",
    "    for i, div in enumerate(soup.find_all('div', class_='sts-tbx-sec')):\n",
    "        div_id = div.get('id')\n",
    "        placeholder = f\"XXXDIVHEREXXX_{i:04d}\"\n",
    "        div_id_placeholders.append((placeholder, div_id))\n",
    "        div.insert_before(placeholder)\n",
    "        div.unwrap()  # Remove the original div, keeping the content\n",
    "\n",
    "    return str(soup), div_id_placeholders\n",
    "\n",
    "def fix_bibliography_bolding(md_text):\n",
    "    lines = md_text.split('\\n')\n",
    "    fixed_lines = []\n",
    "    in_bib = False\n",
    "    bib_started = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().lower() == '## bibliography':\n",
    "            in_bib = True\n",
    "            bib_started = False\n",
    "            fixed_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        if in_bib:\n",
    "            # Stop when next heading is found\n",
    "            if line.startswith('#'):\n",
    "                in_bib = False\n",
    "                fixed_lines.append(line)\n",
    "                continue\n",
    "\n",
    "            # Skip the second line (---|---) if it exists\n",
    "            if not bib_started and re.match(r'^---\\s*\\|\\s*---$', line.strip()):\n",
    "                bib_started = True\n",
    "                continue\n",
    "\n",
    "            # Replace the first | with â\n",
    "            fixed_lines.append(re.sub(r'\\s*\\|\\s*', ' â ', line, count=1))\n",
    "        else:\n",
    "            fixed_lines.append(line)\n",
    "\n",
    "    return '\\n'.join(fixed_lines)\n",
    "\n",
    "def demote_all_headings_but_first(md_text):\n",
    "    lines = md_text.split('\\n')\n",
    "    heading_count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^\\s*#{1,6}\\s', line):\n",
    "            heading_count += 1\n",
    "            if heading_count > 1:\n",
    "                lines[i] = '#' + line  # Add one more #\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def ensure_blank_line_before_headings(md_text):\n",
    "    lines = md_text.split('\\n')\n",
    "    result = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        is_heading = re.match(r'^\\s*#{1,6}\\s', line)\n",
    "        if is_heading:\n",
    "            if i > 0 and lines[i-1].strip() != '':\n",
    "                result.append('')  # Add a blank line\n",
    "        result.append(line)\n",
    "\n",
    "    return '\\n'.join(result)\n",
    "\n",
    "def ensure_blank_lines_around_images(markdown):\n",
    "    lines = markdown.splitlines()\n",
    "    updated_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        is_image_line = re.match(r'!\\[.*?\\]\\(.*?\\)', line.strip())\n",
    "\n",
    "        if is_image_line:\n",
    "            # Add blank line before if needed\n",
    "            if updated_lines and updated_lines[-1].strip() != '':\n",
    "                updated_lines.append('')  # insert blank line before image\n",
    "\n",
    "            updated_lines.append(line)\n",
    "\n",
    "            # Check next line â if it's not blank, insert a blank\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip() != '':\n",
    "                updated_lines.append('')\n",
    "            i += 1\n",
    "        else:\n",
    "            updated_lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    return '\\n'.join(updated_lines)\n",
    "\n",
    "def convert_starred_lettered_to_clean_list(markdown):\n",
    "    lines = markdown.splitlines()\n",
    "    converted_lines = []\n",
    "    pattern = re.compile(r'^\\s*\\*\\s*([a-zA-Z])\\)\\s+(.*)')\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            letter, content = match.groups()\n",
    "\n",
    "            # Insert a blank line before if previous line is not already blank\n",
    "            if converted_lines and converted_lines[-1].strip() != '':\n",
    "                converted_lines.append('')\n",
    "\n",
    "            converted_lines.append(f\"{letter}) {content}\")\n",
    "        else:\n",
    "            converted_lines.append(line)\n",
    "\n",
    "    return '\\n'.join(converted_lines)\n",
    "\n",
    "def link_bibliography_references(markdown_text):\n",
    "    lines = markdown_text.splitlines()\n",
    "    bib_start = None\n",
    "\n",
    "    # Step 1: Find the start of the bibliography\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().lower().startswith(\"### bibliography\"):\n",
    "            bib_start = i\n",
    "            break\n",
    "\n",
    "    if bib_start is None:\n",
    "        raise ValueError(\"Bibliography section not found.\")\n",
    "\n",
    "    # Step 2: Modify the bibliography lines to include anchors\n",
    "    for i in range(bib_start + 1, len(lines)):\n",
    "        line = lines[i]\n",
    "        match = re.match(r'^\\[(\\d+)]\\s*â', line)\n",
    "        if match:\n",
    "            ref_num = match.group(1)\n",
    "            rest = line.split(\"â\", 1)[1].strip()\n",
    "            lines[i] = f'<div id=\"ref-{ref_num}\">[{ref_num}] â {rest}</div>'\n",
    "\n",
    "    # Step 3: Replace in-text [n] with clickable HTML anchor\n",
    "    def replace_citation(match):\n",
    "        num = match.group(1)\n",
    "        return f'<a href=\"#ref-{num}\">[{num}]</a>'\n",
    "\n",
    "    updated_text = '\\n'.join(lines[:bib_start])\n",
    "    updated_text = re.sub(r'\\[(\\d+)]', replace_citation, updated_text)\n",
    "\n",
    "    # Combine updated in-text and modified bib\n",
    "    final_text = updated_text + '\\n' + '\\n'.join(lines[bib_start:])\n",
    "    return final_text\n",
    "\n",
    "def demote_bibliography_heading(markdown_text):\n",
    "    lines = markdown_text.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^###\\s+Bibliography$', line.strip()):\n",
    "            lines[i] = '## Bibliography'\n",
    "            break  # Only the first match\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def preserve_visual_line_breaks(markdown_text):\n",
    "    lines = markdown_text.splitlines()\n",
    "    processed_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "\n",
    "        is_last_line = (i == len(lines) - 1)\n",
    "        next_line_blank = (not is_last_line and lines[i+1].strip() == '')\n",
    "\n",
    "        # Skip block-level elements and leave them unchanged\n",
    "        if (\n",
    "            stripped == '' or\n",
    "            stripped.startswith('#') or\n",
    "            stripped.startswith('<') or\n",
    "            re.match(r'^[-*+]\\s', stripped) or\n",
    "            re.match(r'^\\d+\\.', stripped) or\n",
    "            re.match(r'^---+$', stripped)\n",
    "        ):\n",
    "            processed_lines.append(line)\n",
    "        else:\n",
    "            # Only force line break if next line is NOT blank (i.e. same paragraph)\n",
    "            if not next_line_blank:\n",
    "                processed_lines.append(line.rstrip() + '  ')\n",
    "            else:\n",
    "                processed_lines.append(line)\n",
    "\n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "def bold_first_term_in_subsections(md_text):\n",
    "    lines = md_text.splitlines()\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        if re.match(r\"^####\\s+3\\.\\d+\\.\\d+\", lines[i].strip()):\n",
    "            j = i + 1\n",
    "            while j < len(lines):\n",
    "                if lines[j].strip() and not lines[j].lstrip().startswith(\"<\") and not lines[j].lstrip().startswith(\"#\"):\n",
    "                    if not lines[j].lstrip().startswith(\"**\"):\n",
    "                        leading_spaces = len(lines[j]) - len(lines[j].lstrip())\n",
    "                        stripped = lines[j].strip()\n",
    "                        lines[j] = \" \" * leading_spaces + f\"**{stripped}**  \"\n",
    "                    break\n",
    "                j += 1\n",
    "        i += 1\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def replace_dcmi_terms_url(md_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Find any URL that contains ...dublin-core/dcmi-terms/... (allowing Unicode hyphen variants)\n",
    "    and replace just that URL with the canonical DCMI Terms URL, preserving trailing punctuation.\n",
    "    \"\"\"\n",
    "    CANON = \"https://www.dublincore.org/specifications/dublin-core/dcmi-terms/2020-01-20/\"\n",
    "\n",
    "    # Accept ASCII hyphen and common Unicode hyphen/dash variants\n",
    "    H = r\"\\-\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212\"\n",
    "\n",
    "    # Match a URL, require the path to contain dublin{H}core/dcmi{H}terms/, stop before ) ] \" > or whitespace\n",
    "    pattern = re.compile(\n",
    "        rf'(http?://[^\\s<>\"\\)\\]]*dublin[{H}]core/dcmi[{H}]terms/[^\\s<>\"\\)\\]]*)(?P<punc>[.,;:]?)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    return pattern.sub(lambda m: CANON + (m.group('punc') or ''), md_text)\n",
    "\n",
    "# Step 1: Read HTML\n",
    "with open(\"docs/p1_clean.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Step 2: Preprocess HTML and capture placeholders\n",
    "patched_html, div_placeholders = preprocess_html(html)\n",
    "\n",
    "# Step 3: Convert to Markdown\n",
    "converter = html2text.HTML2Text()\n",
    "converter.ignore_links = False\n",
    "converter.body_width = 0\n",
    "converter.single_line_break = True\n",
    "markdown = converter.handle(patched_html)\n",
    "\n",
    "# Step 4: Postprocess\n",
    "for placeholder, div_id in div_placeholders:\n",
    "    div_html = f'<div class=\"sts-tbx-sec\" id=\"{div_id}\"></div>'\n",
    "    markdown = markdown.replace(placeholder, div_html)\n",
    "\n",
    "markdown = fix_bibliography_bolding(markdown)\n",
    "markdown = demote_all_headings_but_first(markdown)\n",
    "markdown = ensure_blank_line_before_headings(markdown)\n",
    "markdown = ensure_blank_lines_around_images(markdown)\n",
    "markdown = convert_starred_lettered_to_clean_list(markdown)\n",
    "markdown = link_bibliography_references(markdown)\n",
    "markdown = demote_bibliography_heading(markdown)\n",
    "markdown = preserve_visual_line_breaks(markdown)\n",
    "markdown = bold_first_term_in_subsections(markdown)\n",
    "markdown = replace_dcmi_terms_url(markdown)\n",
    "\n",
    "# Step 5: Write final Markdown\n",
    "with open(\"docs/p1_clean.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb02bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed docs/p1_clean.md -> docs/index.md\n",
      "docs/p1_clean.md not found, skipping delete.\n",
      "Deleted docs/p1.html\n",
      "Deleted docs/p1_clean.html\n"
     ]
    }
   ],
   "source": [
    "# File cleanup and renaming\n",
    "\n",
    "# Paths\n",
    "old_file = \"docs/p1_clean.md\"\n",
    "new_file = \"docs/index.md\"\n",
    "files_to_delete = [\n",
    "    \"docs/p1_clean.md\",\n",
    "    \"docs/p1.html\",\n",
    "    \"docs/p1_clean.html\"\n",
    "]\n",
    "\n",
    "# Rename p1_clean.md -> index.md\n",
    "if os.path.exists(old_file):\n",
    "    os.rename(old_file, new_file)\n",
    "    print(f\"Renamed {old_file} -> {new_file}\")\n",
    "else:\n",
    "    print(f\"{old_file} not found, skipping rename.\")\n",
    "\n",
    "# Delete specified files\n",
    "for file_path in files_to_delete:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted {file_path}\")\n",
    "    else:\n",
    "        print(f\"{file_path} not found, skipping delete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
